{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Yuan(Brian) Hu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using Multinomial Naive Bayes on 20newsgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   In this notebook, I will go through text classification task by implementing ``Multinomial Naive Bayes Classifier`` from scratch, the data can be downloaded from [20news-bydate-matlab.tgz](http://qwone.com/~jason/20Newsgroups/20news-bydate-matlab.tgz) and [vocabulary.txt](http://qwone.com/~jason/20Newsgroups/vocabulary.txt). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Objectives and deliverables:\n",
    "- How does Multinomial Naive Bayes Classifier work?\n",
    "- Class implementation of ``Multinomial Naive Bayes Classifier``\n",
    "- Improve model performance(Accuracy) by replacing ``term-frequency`` f with log(1+f) and introducing ``TF-IDF``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> Multinomial Navie Bayes in text classfication</div>\n",
    "\n",
    "#### ``Multinomial`` distribution over a vocabulary $V$:\n",
    "\n",
    "<div align=\"center\">$p = (p_1,...,p_{|V|})$, such that $p_i\\geq  0$ and $\\sum_i P_i =1$</div>\n",
    "\n",
    "#### Document $x = (w_1,...w_{|V|})$ has probability     $\\propto p_1^{w_1}p_2^{w_2} \\cdot \\cdot \\cdot p_{|V|}^{w_{|V|}}$\n",
    "\n",
    "#### Fitting navie Bayes: one multinomial distribution per class.\n",
    "- ``Prior probabilities``: Class probabilities $\\pi_1, ... , \\pi_k$\n",
    "- ``Likelihood``: Multinomials $p^1 = (p_{11},...,p_{1|V|})$, ... , $p^k = (p_{k1},...,p_{j|V|})$\n",
    "\n",
    "#### Classifying document x by ``MAP estimator``:\n",
    "<div align=\"center\">$\\underset{j}{\\operatorname{argmax}} \\pi_j \\Pi^{|V|}_{i=1} P_{ji}^{X_i}$</div>\n",
    "equals to:\n",
    "\n",
    "<div align=\"center\">$\\underset{j}{\\operatorname{argmax}} log(\\pi_j)+\\sum^{|V|}_{i}X_i \\cdot log P_j(X_i)$</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words in the documents constitute an overall vocabulary V of size 61188. All the documents will have to be classified into 20 classes. For each of the 20 classes $j = 1, 2, . . . , 20$, we are going to calculate the following:\n",
    "\n",
    "- $\\pi_j$ , the fraction of documents that belong to that class; and\n",
    "- $P_j(X)$ , a probability distribution over V that models the documents of that class.\n",
    "\n",
    "In order to fit $P_j(X)$ , imagine that all the documents of class $j$ are strung together. For each word\n",
    "$w \\in V$ in the document $X_i$ , let $P_j(X_i)$ be the fraction of this concatenated document occupied by $w$. We\n",
    "will need to do smoothing (just add one to the count of how often $w$ occurs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Flows:\n",
    "\n",
    "\n",
    " 1. Load training/testing data into DataFrame\n",
    " \n",
    "\n",
    " 2. Transform training/testing data to get ``BOW(Bag-of-Words)``matrix\n",
    " \n",
    "\n",
    " 3. Estimate ``prior probability`` $\\pi_j$ and ``likelihood probability`` $P_j(X_i)$ (multinomials)\n",
    " \n",
    " \n",
    " 4. Classify documents $X_i$ by estimating $\\underset{j}{\\operatorname{argmax}} log(\\pi_j)+\\sum^{|V|}_{i}X_i* log P_j(X_i)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load data into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data_path, label_path):\n",
    "    #process labels\n",
    "    labels = pd.read_csv(label_path, header = None)\n",
    "    labels = labels.rename(columns={0: \"Class\"})\n",
    "    labels.index = labels.index + 1\n",
    "    labels.index.names = ['docIdx']\n",
    "    \n",
    "    #process data\n",
    "    data = pd.read_csv(data_path, delimiter = \" \", header = None)\n",
    "    data.rename(columns={0: \"docIdx\", 1: \"wordIdx\", 2: \"Count\"}, inplace=True) \n",
    "    \n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def count_V(filepath):\n",
    "    f = open(filepath, 'r')\n",
    "    words = f.readlines()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess training data\n",
    "train_data = 'data/20news-bydate 2/matlab/train.data'\n",
    "train_label = 'data/20news-bydate 2/matlab/train.label'\n",
    "\n",
    "data, labels = data_processing(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge data with labels\n",
    "data = data.merge(labels, how='left', on='docIdx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>docIdx</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wordIdx</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1   2   3   4   5   6   7   8   9   ...  20  21  22  23  24  25  \\\n",
       "docIdx    1   1   1   1   1   1   1   1   1   1  ...   1   1   1   1   1   1   \n",
       "wordIdx   1   2   3   4   5   6   7   8   9  10  ...  21  22  23  24  25  26   \n",
       "Count     4   2  10   4   2   1   1   1   3   9  ...   1   1  54   3   3   1   \n",
       "Class     1   1   1   1   1   1   1   1   1   1  ...   1   1   1   1   1   1   \n",
       "\n",
       "         26  27  28  29  \n",
       "docIdx    1   1   1   1  \n",
       "wordIdx  27  28  29  30  \n",
       "Count    11   2  90  20  \n",
       "Class     1   1   1   1  \n",
       "\n",
       "[4 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           1\n",
       "1           1\n",
       "2           1\n",
       "3           1\n",
       "4           1\n",
       "           ..\n",
       "1467133    20\n",
       "1467134    20\n",
       "1467135    20\n",
       "1467136    20\n",
       "1467137    20\n",
       "Name: Class, Length: 56315, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if docIdx gets correct Class label\n",
    "data.groupby(\"docIdx\").Class.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess testing data\n",
    "test_data = 'data/20news-bydate 2/matlab/test.data'\n",
    "test_labels = 'data/20news-bydate 2/matlab/test.label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, test_labels = data_processing(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Transform DataFrames to get ``Bag-of-Words``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``The reason Naive Bayes is naive``, is that it does not take word order or phrasing into account.\n",
    "\n",
    "\"In other words, Naive Bayes would give the exact same probability to the phrase \"I like \n",
    "pizza\" as it would to the phrase \"Pizza like I\". Even though people frequently say \"I like pizza\" and almost never say \"Pizza like I\". Because keeping track of every phrase and word ordering would be impossible, Naive Bayes doesn’t even try. That said, Naive Bayes works well in practice, so keeping track of word order must not be super important.\"   -StatsQuest\n",
    "\n",
    "In regarding to likelihood of each word, it makes ``naive`` assumption that each word are uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build a ``Bag of Words`` matrix from our DataFrame for the following modeling steps, first, we need acquire the length of Vocabulary and the number of documents respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the outputs of this step as our ``X_train`` and ``X_test`` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11269"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of documents - as number of samples(index starts from 1 to 11269)\n",
    "n_docs = data.docIdx.nunique()\n",
    "n_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words in vocabulary.txt is 61188\n"
     ]
    }
   ],
   "source": [
    "#number of words\n",
    "V_path = 'data/vocabulary.txt'\n",
    "\n",
    "n_words = count_V(V_path)\n",
    "print(f\"The total number of words in vocabulary.txt is {n_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(df, n_words):\n",
    "    \"\"\"\n",
    "    dataframe: DataFrame(\"docIdx\",\"wordIdx\",\"Count\",\"Class\")\n",
    "    n_words: the number of unique words in \"vocabulary.txt\"\n",
    "    \n",
    "    word_mat: word count matrix of shape (n_documents, n_words)\n",
    "              this result also will serve us as X_train\n",
    "    \"\"\"\n",
    "    n_docs = df.docIdx.nunique()\n",
    "\n",
    "    word_mat = np.zeros((n_docs, n_words))\n",
    "    for i in range(n_docs):\n",
    "        doc = df[df.docIdx==i+1]\n",
    "        word_vec = np.zeros((n_words, ))\n",
    "        mask = doc.wordIdx.values-1\n",
    "        word_vec[mask] = doc.Count.values\n",
    "        word_mat[i]=word_vec\n",
    "        \n",
    "    return word_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  2., 10., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize X_train\n",
    "X_train = bag_of_words(data, n_words)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11269, 61188)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 13. 15. 19. 20. 24. 27.\n",
      " 54. 58. 90.]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 13 15 19 20 24 27 54 58 90]\n"
     ]
    }
   ],
   "source": [
    "#checking the matrix correctly reflects dataframe's values\n",
    "print(np.unique(X_train[0]))\n",
    "print(np.unique(data[data.docIdx==1].Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will list all the words with zero count in the matrix, the results above matches our expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, ..., 20, 20, 20])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize y_train\n",
    "y_train = labels.Class.values\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of classes\n",
    "n_classes = np.unique(y_train).shape[0]\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform testing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7505, 61188)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize X_test\n",
    "X_test = bag_of_words(test, n_words)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 12.]\n",
      "[ 1  2  3  4  5  6  7  8  9 10 12]\n"
     ]
    }
   ],
   "source": [
    "#checking the matrix correctly reflects dataframe's values\n",
    "print(np.unique(X_test[0]))\n",
    "print(np.unique(test[test.docIdx==1].Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, ..., 20, 20, 20])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize y_test\n",
    "y_test = test_labels.Class.values\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Estimate ``prior probability`` $\\pi_j$ and ``likelihood probability`` $P_j(X_i)$ (multinomials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate ``prior probability`` $\\pi_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sum of all the prior probabilities should equals to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([480, 581, 572, 587, 575, 592, 582, 592, 596, 594, 598, 594, 591,\n",
       "       594, 593, 599, 545, 564, 464, 376])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of documents in each class\n",
    "n_docs_class = np.array([y_train[y_train==i].shape[0] for i in range(1, 21)])\n",
    "n_docs_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the fraction of documents that belong to that class\n",
    "prior = n_docs_class/np.sum(n_docs_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04259473, 0.05155737, 0.05075872, 0.0520898 , 0.05102494,\n",
       "       0.0525335 , 0.05164611, 0.0525335 , 0.05288846, 0.05271098,\n",
       "       0.05306593, 0.05271098, 0.05244476, 0.05271098, 0.05262224,\n",
       "       0.05315467, 0.04836277, 0.05004881, 0.0411749 , 0.03336587])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check constraint\n",
    "np.sum(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04259473, 0.05155737, 0.05075872, 0.0520898 , 0.05102494,\n",
       "       0.0525335 , 0.05164611, 0.0525335 , 0.05288846, 0.05271098,\n",
       "       0.05306593, 0.05271098, 0.05244476, 0.05271098, 0.05262224,\n",
       "       0.05315467, 0.04836277, 0.05004881, 0.0411749 , 0.03336587])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternative with np.unique()\n",
    "n,y_counts = np.unique(y_train, return_counts=True)\n",
    "#print(n,y_counts)\n",
    "phi_y =  y_counts/y_counts.sum()\n",
    "phi_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate ``likelihood probability`` $P_j(X_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Constraint: Sum of likelihood probabilities of each class should equals to 1 as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ``Smoothing``: we will use add-one smoothing, which sets alpha to be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(X_j|w_i)=\\frac{\\sum^{|V|}_{i}tf(X_j, w_i \\in d_j) + \\alpha}{\\sum^{|V|}_{i}N+ \\alpha*|V|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate multinomial probability of each word within each document class\n",
    "#returns a matrix which has the same shape with tf matrix\n",
    "def multinomials(X_train, y_train):\n",
    "    tf = np.zeros((n_classes, n_words))\n",
    "    #X_train = X_train + 1\n",
    "    for j in range(20):\n",
    "        jth_class = X_train[y_train==j+1]\n",
    "        f = (np.sum(jth_class, axis=0)+1)/(np.sum(jth_class)+n_words)\n",
    "        tf[j] = f\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = multinomials(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.66666667e-05, 3.04761905e-04, 1.31428571e-03, ...,\n",
       "        4.76190476e-06, 4.76190476e-06, 4.76190476e-06],\n",
       "       [3.55589754e-04, 3.49760414e-04, 5.82934024e-06, ...,\n",
       "        5.82934024e-06, 5.82934024e-06, 5.82934024e-06],\n",
       "       [7.89707479e-05, 4.60662696e-04, 6.58089566e-06, ...,\n",
       "        6.58089566e-06, 6.58089566e-06, 6.58089566e-06],\n",
       "       ...,\n",
       "       [3.48108977e-05, 4.90517195e-04, 3.16462706e-06, ...,\n",
       "        3.16462706e-06, 3.16462706e-06, 3.16462706e-06],\n",
       "       [4.03854386e-06, 1.61541755e-04, 4.03854386e-06, ...,\n",
       "        4.03854386e-06, 4.03854386e-06, 4.03854386e-06],\n",
       "       [5.54680393e-06, 2.55152981e-04, 5.54680393e-05, ...,\n",
       "        5.54680393e-06, 5.54680393e-06, 5.54680393e-06]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check constraint \n",
    "np.sum(likelihood, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Estimate \n",
    "<div align=\"center\">$\\underset{j}{\\operatorname{argmax}} log(\\pi_j)+\\sum^{|V|}_{i}X_i* log P_j(X_i)$</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of classify a single sample X_test[0]\n",
    "probs = []\n",
    "for j in range(20):\n",
    "    prob = np.sum(X_test[2]*np.log(likelihood[j]))+np.log(prior[j])\n",
    "    probs.append(prob)\n",
    "    \n",
    "np.argmax(probs)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For test sample X_test[0], the predicted result is class\"1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the logic above we write our estimator function, we will take advantage of numpy broadcasting for 2D-matrix computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate optimal solution for estimator\n",
    "def estimator(X_test, prior, likelihood):\n",
    "    if X_test.ndim == 1:\n",
    "        n_documents = 1\n",
    "    else:\n",
    "        n_documents = X_test.shape[0]\n",
    "\n",
    "    labels = np.zeros((n_documents, ))\n",
    "    for i in range(n_documents):\n",
    "        log_prob = np.sum(np.log(likelihood)*X_test[i], axis=1) + np.log(prior)\n",
    "        labels[i] = np.argmax(log_prob)+1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Improving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we wrap up the class implementation, we will take these two strategies to try to imporve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting training data into training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_, X_valid, y_train_, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9015, 61188)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9015,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs_class_ = np.array([y_train_[y_train_==i].shape[0] for i in range(1, 21)])\n",
    "prior_ = n_docs_class_/np.sum(n_docs_class_)\n",
    "likelihood_ = multinomials(X_train_, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_valid_base = estimator(X_valid, prior_, likelihood_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on 2254 samples, Accuracy:0.84\n"
     ]
    }
   ],
   "source": [
    "acc_base = X_pred_valid_base[X_pred_valid_base==y_valid].shape[0]/y_valid.shape[0]\n",
    "print(f\"Predicting on {y_valid.shape[0]} samples, Accuracy:{round(acc_base, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we don't remove stopwords here simply because the vocabulary has already eliminated common stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### strategy 1: replacing the frequency f of a word in a document by log(1+f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tf(t,d) = log (1 + f_{t,d})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a word appears again, the probability of it appearing again goes up. In order to smooth this, we take the log of the frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomials_revised(X_train, y_train):\n",
    "    tf = np.zeros((n_classes, n_words))\n",
    "    #X_train = X_train + 1\n",
    "    for j in range(20):\n",
    "        jth_class = X_train[y_train==j+1]\n",
    "        f = (np.sum(jth_class, axis=0)+1)/(np.sum(jth_class) + n_words)\n",
    "        tf[j] = np.log(1+f)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = multinomials_revised(X_train_, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_valid_1 = estimator(X_valid, prior_, log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on 2254 samples, Accuracy:0.84\n"
     ]
    }
   ],
   "source": [
    "acc_1 = X_pred_valid_1[X_pred_valid_1==y_valid].shape[0]/y_valid.shape[0]\n",
    "print(f\"Predicting on {y_valid.shape[0]} samples, Accuracy:{round(acc_1, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### strategy 2: introducing ``Inverse Document Frequency``(IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. It leverage the importance of some low-frequency words compared to high-frequency words with little relavence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$idf(t, D) = log \\frac{N}{1+ |\\{d\\in D: t\\in d\\}|}$$\n",
    "\n",
    "N: total number of documents in the corpus $N=|D|$\n",
    "\n",
    "$|\\{d\\in D: t\\in d\\}|$: number of documents where the term t appears. If the term is not in the corpus, this will lead to a division by zero. Therefore it is common to adjust the denominator to $1+|\\{d\\in D: t\\in d\\}|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Tfidf = tf(t, D)*idf(t, D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([376, 471, 447, 465, 462, 461, 468, 467, 490, 479, 486, 480, 471,\n",
       "       480, 484, 473, 432, 447, 377, 299])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of documents\n",
    "N = np.array([X_train_[y_train_==i+1].shape[0] for i in range(20)])\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.count_nonzero(D, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate idf \n",
    "def inverse_term_frequency(X, y, n_words):\n",
    "    n_classes = np.unique(y).shape[0]\n",
    "\n",
    "    idf = np.zeros((n_classes, n_words))\n",
    "    for i in range(20):\n",
    "        D = X[y==i+1]\n",
    "        N = np.array(D.shape[0])\n",
    "        df = np.count_nonzero(D, axis=0) + 1 #add 1 for smoothing\n",
    "        idf[i] = np.log(N/df)\n",
    "        \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = inverse_term_frequency(X_train_, y_train_, n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = multinomials(X_train_, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.12860513e-04, 6.30249245e-04, 2.35394742e-03, ...,\n",
       "        3.47263232e-05, 3.47263232e-05, 3.47263232e-05],\n",
       "       [1.44526387e-03, 9.72199866e-04, 3.97351666e-05, ...,\n",
       "        3.97351666e-05, 3.97351666e-05, 3.97351666e-05],\n",
       "       [3.21619091e-04, 9.96811332e-04, 4.52111706e-05, ...,\n",
       "        4.52111706e-05, 4.52111706e-05, 4.52111706e-05],\n",
       "       ...,\n",
       "       [1.44818061e-04, 8.78946355e-04, 2.36237524e-05, ...,\n",
       "        2.36237524e-05, 2.36237524e-05, 2.36237524e-05],\n",
       "       [2.92815901e-05, 4.00338502e-04, 2.92815901e-05, ...,\n",
       "        2.92815901e-05, 2.92815901e-05, 2.92815901e-05],\n",
       "       [3.68731634e-05, 4.63853338e-04, 1.94288775e-04, ...,\n",
       "        3.68731634e-05, 3.68731634e-05, 3.68731634e-05]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate optimal solution for estimator\n",
    "def estimator_tfidf(X_test, prior, tfidf):\n",
    "    if X_test.ndim == 1:\n",
    "        n_documents = 1\n",
    "    else:\n",
    "        n_documents = X_test.shape[0]\n",
    "\n",
    "    labels = np.zeros((n_documents, ))\n",
    "    for i in range(n_documents):\n",
    "        log_prob = np.sum(np.log(tfidf)*X_test[i], axis=1) + np.log(prior)\n",
    "        labels[i] = np.argmax(log_prob)+1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_pred_2 = estimator_tfidf(X_valid, prior_, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2254,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on 2254 samples, Accuracy:0.761\n"
     ]
    }
   ],
   "source": [
    "acc_2 = X_pred_2[X_pred_2==y_valid].shape[0]/y_valid.shape[0]\n",
    "print(f\"Predicting on {y_valid.shape[0]} samples, Accuracy:{round(acc_2, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's no improvement by adding idf, this perhaps because the vocabulary set being dealing with has already eliminated all the high-frequency words that has little relavence to the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wraps everything up into class implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class implementation of Naive-Bayes Multinomial Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes_clf():\n",
    "    \n",
    "    def __init__(self, alpha=1, tf='f', tfidf=False):\n",
    "        self.alpha = alpha\n",
    "        self.tf = tf\n",
    "        self.tfidf = tfidf\n",
    "        print(f\"Naive-Bayes-Multinomial-Classifier: alpha={self.alpha}, tf={self.tf}, tfidf={self.tfidf}\")\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = np.unique(y).shape[0]\n",
    "        self.n_docs = X.shape[0]\n",
    "        self.n_words = X.shape[1]\n",
    "        \n",
    "        #vector(20,), number of documents in each class\n",
    "        n_docs_class = np.array([y[y==j].shape[0] for j in range(1, self.n_classes+1)]) #class starts from 1\n",
    "        self.prior = n_docs_class/np.sum(n_docs_class)\n",
    "        \n",
    "        self.likelihood = self._multinomials(X, y)\n",
    "        \n",
    "    def _multinomials(self, X, y):\n",
    "        tf = np.zeros((n_classes, n_words))\n",
    "\n",
    "        for j in range(20):\n",
    "            jth_class = X[y==j+1]\n",
    "            f = (np.sum(jth_class, axis=0) + self.alpha)/(np.sum(jth_class) + self.alpha*n_words)\n",
    "            if self.tf == 'f':\n",
    "                tf[j] = f\n",
    "            else:\n",
    "                tf[j] = np.log(1+f)\n",
    "        return tf\n",
    "    \n",
    "\n",
    "    # calculate optimal solution for estimator\n",
    "    def predict(self, X):\n",
    "        if X_test.ndim == 1:\n",
    "            n_test_samples = 1\n",
    "        else:\n",
    "            n_test_samples = X_test.shape[0]\n",
    "\n",
    "        labels = np.zeros((n_test_samples, ))\n",
    "        for i in range(n_test_samples):\n",
    "            log_prob = np.sum(np.log(self.likelihood)*X[i], axis=1) + np.log(self.prior)\n",
    "            labels[i] = np.argmax(log_prob)+1\n",
    "        return labels\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        acc = X[X==y].shape[0]/y.shape[0]\n",
    "        print(f\"Predicting on {y.shape[0]} samples, Accuracy:{round(acc, 3)}\")\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions on test set with baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive-Bayes-Multinomial-Classifier: alpha=1, tf=f, tfidf=False\n",
      "Predicting on 7505 samples, Accuracy:0.781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7810792804796802"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_base = NaiveBayes_clf()\n",
    "clf_base.fit(X_train, y_train)\n",
    "X_pred_base = clf_base.predict(X_test)\n",
    "clf_base.accuracy(X_pred_base, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions on test set with replaced log(1+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive-Bayes-Multinomial-Classifier: alpha=1, tf=log, tfidf=False\n",
      "Predicting on 7505 samples, Accuracy:0.781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7814790139906729"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_log = NaiveBayes_clf(tf='log')\n",
    "clf_log.fit(X_train, y_train)\n",
    "X_pred_log = clf_log.predict(X_test)\n",
    "clf_log.accuracy(X_pred_log, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
