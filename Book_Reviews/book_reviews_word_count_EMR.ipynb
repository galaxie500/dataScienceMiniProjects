{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83f5db0",
   "metadata": {},
   "source": [
    "# Book Reviews - Word Count with pyspark on Amazon EMR\n",
    "\n",
    "---\n",
    "\n",
    "#### Tasks:\n",
    "\n",
    "- Work with **BookReviews_1M** dataset \n",
    "- Find top 100 words and their counts based on word count for the **BookReviews_5M** dataset\n",
    "- Calculate average and standard deviation of execution times over 3 runs for these three settings:\n",
    "    1. BookReviews_1M - 1 master + 1 worker node \n",
    "    2. BoookReviews_5M - 1 master + 1 worker node\n",
    "    3. BookReviews_5M - 1 master + 3 worker nodes\n",
    "    \n",
    "    Note that worker nodes are also called core nodes when initializing them on AWS.\n",
    "--- \n",
    "\n",
    "PySpark API Documentation: https://spark.apache.org/docs/latest/api/python/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a6c22",
   "metadata": {},
   "source": [
    "### 1. Upload the 1M dataset to S3\n",
    "\n",
    "To make the datasets available to the EMR cluster, we need to upload the data files to Amazon S3. Follow these steps to do so:\n",
    "\n",
    "1. In the Amazon console, open the **Services** menu on the top left and select **S3**\n",
    "2. Create a bucket if you don't have one yet. Use the default settings, but your bucket name must be unique. \n",
    "3. Create a folder in your bucket, e.g. `data`, using the default settings. (Don't upload the data file to the root of the bucket; we'll also use this bucket for later assignments, so it's good to keep everything organized.)\n",
    "4. Enter the folder and upload the **.txt** file. Do NOT upload the zip, as Spark won't know what to do with it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850e9b2",
   "metadata": {},
   "source": [
    "### 2. Setting up the EMR cluster and creating a PySpark notebook\n",
    "\n",
    "We have already uploaded the 5M reviews data to the s3 bucket `s3://dse230-emr`. Follow the steps below to create an EMR cluster:\n",
    "\n",
    "1. In AWS, go to Services -> EMR.\n",
    "2. Click on 'Create cluster'.\n",
    "3. Click on 'Go to advanced options'.\n",
    "4. Select the EMR version 6.2.0, add required software packages as shown in class.\n",
    "5. Specify the instance count for master and core nodes\n",
    "6. Give your cluster a name, select an EC2 keypair that you should have created earlier. If you have not created an EC2 keypair, stop here. Go back and create a keypair first, then come back to this step.\n",
    "7. Proceed to create a cluster and wait for completion. This will take a few minutes (typically ~5-8 minutes).\n",
    "8. Go to `Notebooks` section in the sidebar of the EMR dashboard page, create a new notebook and attach it to the cluster you created earlier\n",
    "9. Open JupyterLab from this page and create a **PySpark** notebook\n",
    "8. The data is at `s3://dse230-emr/BookReviews_5M.txt`. In the following sections, use this URI for data file path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2461fac",
   "metadata": {},
   "source": [
    "### 3. Start Spark Session\n",
    "\n",
    "Note that yo don't need to manually start the spark session. AWS does it for you in the background, so that the spark session is started as soon as you import pyspark. The spark session is automatically available in the global variable `spark`\n",
    "\n",
    "Remember that the kernel for running this Notebook is **PySpark** and not Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a29d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcee356ee084ccb834097c2641b50f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>12</td><td>application_1621983455664_0013</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-29-6.us-west-2.compute.internal:20888/proxy/application_1621983455664_0013/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-25-189.us-west-2.compute.internal:8042/node/containerlogs/container_1621983455664_0013_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1-amzn-0 3.0.1+amzn.0"
     ]
    }
   ],
   "source": [
    "# Initialize Spark\n",
    "import pyspark\n",
    "\n",
    "print (spark.version, pyspark.version.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5738b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d45806201b347d2bb3608abf016aece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record the starting time of execution for timing this notebook\n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4581e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252da8f09b5c4dffb8fe0546584327bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read data from HDFS or S3 - For the purposes of this assignment, you should read data from HDFS\n",
    "# Although you can read directly from S3 theoretically.\n",
    "\n",
    "# Provide the HDFS file path of the 5M dataset.\n",
    "# \"s3://dse230-emr/BookReviews_5M.txt\"\n",
    "# \"s3://dse230yuh053/data/BookReviews_1M.txt\"\n",
    "dataFileName = \"s3://dse230-emr/BookReviews_5M.txt\"\n",
    "\n",
    "# Read data from the above file path and convert it to a dataframe. \n",
    "textDF = spark.read.text(dataFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03621653",
   "metadata": {},
   "source": [
    "### 4. Examine the data\n",
    "\n",
    "Your task: \n",
    "1. Examine the contents of the dataframe that you've just read from file.\n",
    "\n",
    "Expected output: \n",
    "1. Print the schema of the raw dataframe, as well as its first 25 rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "237cfda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcdcfb95b7e49f0ac8cc636c5b89dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "None\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|This was the firs...|\n",
      "|Also after going ...|\n",
      "|As with all of Ms...|\n",
      "|I've not read any...|\n",
      "|This romance nove...|\n",
      "|Carolina Garcia A...|\n",
      "|Not only can she ...|\n",
      "|Once again Garcia...|\n",
      "|The timing is jus...|\n",
      "|Engaging. Dark. R...|\n",
      "|Set amid the back...|\n",
      "|This novel is a d...|\n",
      "|If readers are ad...|\n",
      "| Reviewed by Phyllis|\n",
      "|      APOOO BookClub|\n",
      "|A guilty pleasure...|\n",
      "|In the tradition ...|\n",
      "|Beryl Unger, top ...|\n",
      "|What follows is a...|\n",
      "|The book flap say...|\n",
      "|I'd never before ...|\n",
      "|The novel's narra...|\n",
      "|It is centered on...|\n",
      "|If you like moder...|\n",
      "|Beryl Unger is a ...|\n",
      "+--------------------+\n",
      "only showing top 25 rows\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "print(textDF.printSchema())\n",
    "print(textDF.show(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a413cdd",
   "metadata": {},
   "source": [
    "### 5. Clean the data\n",
    "\n",
    "Your task:\n",
    "1. Remove all punctuations and convert all characters to lower case.\n",
    "\n",
    "Expected output:\n",
    "1. The first 25 rows of a dataframe, with a column containing the cleaned sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97baca36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346fc92316134c218da036d9e83d9822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "\n",
    "def removePunctuation(column):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\"\"\"\n",
    "    return trim(lower(regexp_replace(column, \"[^A-Za-z0-9 ]\", \"\"))).alias(\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f87e549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c3b804dca54663aee5f0e6d9d08fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recommended: take a look at the contents of a column object returned from removePunctuations. What's in there? \n",
    "# No answers or outputs required for this cell. \n",
    "textDF = textDF.withColumn('sentence', removePunctuation(textDF.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6839737b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc137327ddb427587e60cf443aba003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               value|            sentence|\n",
      "+--------------------+--------------------+\n",
      "|This was the firs...|this was the firs...|\n",
      "|Also after going ...|also after going ...|\n",
      "|As with all of Ms...|as with all of ms...|\n",
      "|I've not read any...|ive not read any ...|\n",
      "|This romance nove...|this romance nove...|\n",
      "|Carolina Garcia A...|carolina garcia a...|\n",
      "|Not only can she ...|not only can she ...|\n",
      "|Once again Garcia...|once again garcia...|\n",
      "|The timing is jus...|the timing is jus...|\n",
      "|Engaging. Dark. R...|engaging dark rea...|\n",
      "|Set amid the back...|set amid the back...|\n",
      "|This novel is a d...|this novel is a d...|\n",
      "|If readers are ad...|if readers are ad...|\n",
      "| Reviewed by Phyllis| reviewed by phyllis|\n",
      "|      APOOO BookClub|      apooo bookclub|\n",
      "|A guilty pleasure...|a guilty pleasure...|\n",
      "|In the tradition ...|in the tradition ...|\n",
      "|Beryl Unger, top ...|beryl unger top e...|\n",
      "|What follows is a...|what follows is a...|\n",
      "|The book flap say...|the book flap say...|\n",
      "|I'd never before ...|id never before r...|\n",
      "|The novel's narra...|the novels narrat...|\n",
      "|It is centered on...|it is centered on...|\n",
      "|If you like moder...|if you like moder...|\n",
      "|Beryl Unger is a ...|beryl unger is a ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 25 rows\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "# execute the column expressions generated by removePunctuation() to clean the sentences\n",
    "# After that, use the show() function to print the first 25 rows of the dataframe\n",
    "# Hint: you'll need the Column object returned by removePunctuations(). \n",
    "print(textDF.show(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9869c5d7",
   "metadata": {},
   "source": [
    "### 6. Get dataframe containing unique words and their counts\n",
    "\n",
    "Your task:\n",
    "1. Split each sentence into words based on the delimiter space (' ').\n",
    "2. Put each word in each sentence row into their own rows. Put your results into a new dataframe.\n",
    "3. Print out the first 5 rows of the dataframe.\n",
    "\n",
    "\n",
    "1. First 5 rows of the output dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d23dd2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455c69c1a69b4880af43fcfc30febd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            sentence|words|\n",
      "+--------------------+-----+\n",
      "|this was the firs...| this|\n",
      "|this was the firs...|  was|\n",
      "|this was the firs...|  the|\n",
      "|this was the firs...|first|\n",
      "|this was the firs...| time|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "# We assemble the 'split' and 'explode' column expressions, then apply them to the sentence column\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# split 'sentence' into words delimited by ' '\n",
    "textDF = textDF.withColumn('words', F.split(textDF.sentence, ' '))\n",
    "df = textDF.select('sentence', F.explode(textDF.words).alias('words'))\n",
    "print(df.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044cf06",
   "metadata": {},
   "source": [
    "Filter out all empty rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76516e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a94c1dcd0f4162ade61053fe092ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.filter(df.words != '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657ed9a",
   "metadata": {},
   "source": [
    "Group the dataframe by unique words, then count each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a48f12e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dafcd2413404fca978f272a2f163e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.groupBy('words').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b3152",
   "metadata": {},
   "source": [
    "### 7. Sort the word count dataframe in a descending manner.\n",
    "\n",
    "Your task: \n",
    "1. Sort the previous dataframe by the counts column in a descending manner. Put your results into a new dataframe. \n",
    "\n",
    "Expected output:\n",
    "1. First 25 rows of the sorted word count dataframe. The first row would have the maximum count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8689186f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b98e492f9b403c8254c1c264ec8c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|words|   Count|\n",
      "+-----+--------+\n",
      "|  the|10642903|\n",
      "|    i| 6326216|\n",
      "|   to| 5607568|\n",
      "|  and| 5537690|\n",
      "|    a| 5166838|\n",
      "|   it| 4654902|\n",
      "|   is| 3242588|\n",
      "|  for| 2860227|\n",
      "| this| 2845219|\n",
      "|   of| 2782166|\n",
      "|   my| 2319813|\n",
      "|   in| 2147373|\n",
      "| with| 2046990|\n",
      "| that| 1983044|\n",
      "|   on| 1758801|\n",
      "|  you| 1754054|\n",
      "| have| 1632887|\n",
      "|  but| 1508591|\n",
      "|  not| 1460730|\n",
      "|  was| 1434985|\n",
      "|   as| 1185866|\n",
      "|  are| 1007811|\n",
      "|   so|  994529|\n",
      "|great|  988223|\n",
      "| very|  893737|\n",
      "+-----+--------+\n",
      "only showing top 25 rows"
     ]
    }
   ],
   "source": [
    "wordCount = df.select('words', F.col(\"count\").alias(\"Count\")).orderBy('Count', ascending=False)\n",
    "wordCount.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3316ab2",
   "metadata": {},
   "source": [
    "### 8. Record the execution time\n",
    "\n",
    "Your task: \n",
    "1. Print the execution time.\n",
    "\n",
    "Expected output: The execution time. No particular value is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d70d1fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c672c9227e349ffa334f9335e0776d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.212690353393555"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "consumed = end_time - start_time\n",
    "print(consumed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b43f655",
   "metadata": {},
   "source": [
    "### 9. Save the sorted word counts directly to S3 as a CSV file\n",
    "\n",
    "NOTE: Spark uses a distributed memory system, and stores working data in fragments known as \"partitions\". This is advantageous when a Spark cluster spans multiple machines, as each machine will only require part of the working data to do its own job. By default, Spark will save each of these data partitions into a individual file to avoid I/O collisions. We want only one output file, so we'll need to fuse all the data into a single partition first. \n",
    "\n",
    "Your task: \n",
    "1. Coalesce the previous dataframe to one partition. This makes sure that all our results will end up in the same CSV file. \n",
    "2. Save the 1-partition dataframe to S3 using the DataFrame.write.csv() method. Take note to store the file inside S3, at a place that you can remember. The save path should look something like `s3://<your-bucket>/<your-folder>/<your-result-file>.csv`. Change these parameters to point to your bucket and folder.\n",
    "3. Remember to save the csv file along with the header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72421ba2",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "#### You only need to run the section 9 and section 10 once for the 5M dataset.\n",
    "#### Section 11 requires you to run multiple iterations of this Notebook, and for that you can comment out the code in section 9 so that it's easier for you to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55acacf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a640a04011f43898519ddb158499d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save results to S3\n",
    "\n",
    "#wordCount.coalesce(1).write.csv(\"s3://dse230yuh053/pa4/wordcount_runtime.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "125eb2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cb9ec7de9c427bb683ed38782210e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fab7bf",
   "metadata": {},
   "source": [
    "### 10. Download the CSV file from S3 to your local machine and create the expected CSV output file\n",
    "\n",
    "1. Navigate to the S3 folder where you stored your output\n",
    "2. Note the name of this file, it should look something like `part-00000-xx.....xx.csv`. \n",
    "3. Click on this file, it should open the file properties.\n",
    "4. Beside 'Copy S3 URI', click on 'Object actions' and then click on 'Download'.\n",
    "5. After downloading the file, you can rename it to anthing, say `results.csv`. \n",
    "6. We want you to submit a CSV containing the first 101 rows of the results file. To do this, use the command `head -n 101 results.csv > 101_rows.csv` on a terminal. You can also do so manually, since CSV files are in plain text. Remember that we want the first 101 lines which would include the header as well - so basically it is header + 100 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46534fe0",
   "metadata": {},
   "source": [
    "### 11. Execution times on different dataset and settings.\n",
    "\n",
    "You need to experiment with using different number of master and worker nodes for running this whole Jupyter Notebook. You will have to report the execution time of this Notebook as you noted in an earlier section.\n",
    "                                   \n",
    "1. Create a cluster with the required number of master and worker nodes.\n",
    "2. Then go to the Kernel tab in JupyterLab, and do 'Restart and run all cells.'\n",
    "3. You should note the time in the cell just before section 9 - this is the time that it took for all the code to run.\n",
    "4. Then, start a new cluster with a different configuration of master and worker nodes and dataset as expected. Run the Notebook again, and note the execution times.\n",
    "\n",
    "Fill in the times in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8ec8f",
   "metadata": {},
   "source": [
    "| Dataset | #Master Nodes | #Core Nodes | Runtime_1 | Runtime_2 | Runtime_3 | Mean | Std |\n",
    "| :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
    "| 1M | 1 | 1 |33.0405|31.0051 |32.6960 |32.2472 |0.8895 | \n",
    "| 5M | 1 | 1 |71.4685 |73.4798| 73.1384|72.6956 |0.8788 | \n",
    "| 5M | 1 | 3 |44.4445 |44.0781 |43.5556 |44.0261 |0.3647 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d62eea00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70af81e56cd455e94ce8c2d71949e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# a = [33.0405, 31.0051, 32.6960]\n",
    "# c = [44.4445, 44.0781, 43.5556]\n",
    "# b = [71.4685, 73.4798, 73.1384]\n",
    "\n",
    "# print(np.mean(a))\n",
    "# print(np.mean(b))\n",
    "# print(np.mean(c))\n",
    "# print(np.std(a))\n",
    "# print(np.std(b))\n",
    "# print(np.std(c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
